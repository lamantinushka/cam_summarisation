{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamantinushka/cam_summarisation/blob/master/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdRvv6ZgvBg0",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "In this part of the project we investigate potential of GPT-2 architecture in unsupervised summarization task. GPT-2 is a self-attentive models consisting of sequentially stacked transformer decoders. It is a model with 1.5 billion parameters trained for language modeling task in 40GB of Internet text. \n",
        "\n",
        "Firstly we try using GPT-2 with the weights that were released. Then we finetune model on the set of all sentences from the CAM outputs and apply finetuned model.\n",
        "\n",
        "We use four techniques for decoding:\n",
        "- greedy decoding (choose the most probable token on each step)\n",
        "- soft decoding (choose token with predicted probabilities)\n",
        "- beam search (maintain heap of most probable decodings on each step)\n",
        "- **UPD** diverse beam search (penalising hypothsis that are expansions of the same parent node)\n",
        "\n",
        "In all cases we start with the phrase \"\\<object1> is better than \\<object2> because\". We also feed sentences corresponding to the extracted aspects. We rank this sentences by the CAM_score and extract up to 5 sentences with different aspects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EhgjP2KCrTX",
        "colab_type": "text"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHtkjswuotYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "0a61a270-8846-4a56-c34a-1daf453fe5d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTIlUx7apIE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ARkfn91xBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "for line in open('drive/My Drive/summarization/mined_bow_str.json', 'r'):\n",
        "    data.append(json.loads(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCPGBCMpin9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# usefull function that extracts all supporting sentences from the CAM output\n",
        "def write_sentences(sample, sentences = None):\n",
        "    if sentences is None:\n",
        "        sentences = []\n",
        "    for s in sample['object1']['sentences']:\n",
        "        sentences.append(s['text'] + '\\n')\n",
        "    for s in sample['object2']['sentences']:\n",
        "        sentences.append(s['text'] + '\\n')\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtvS2R591A7Z",
        "colab_type": "text"
      },
      "source": [
        "Let's choose some samples to compare model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aCKTgYKFLvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_sentences_with_aspects(sample, n = 5):\n",
        "    sentences = []\n",
        "    aspects = []\n",
        "    for s in sample['object1']['sentences']:\n",
        "        if s['context_aspects'] != [] and s['context_aspects'][0] not in aspects:\n",
        "            sentences.append((s['CAM_score'], s['text'], s['context_aspects'][0]))\n",
        "    for s in sample['object2']['sentences']:\n",
        "        if s['context_aspects'] != [] and s['context_aspects'][0] not in aspects:\n",
        "            sentences.append((s['CAM_score'], s['text'], s['context_aspects'][0]))\n",
        "    sentences = sorted(sentences)\n",
        "    result = []\n",
        "    i = 1\n",
        "    while len(aspects) != 5 and i <= len(sentences):\n",
        "        if sentences[-i][2] not in aspects:\n",
        "            result.append(sentences[-i][1])\n",
        "            aspects.append(sentences[-i][2])\n",
        "        i += 1\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi92QgUZse69",
        "colab_type": "code",
        "outputId": "d5cc08dc-6868-4484-c366-922aa6b2824a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "samples = [data[4], data[155], data[228]]\n",
        "\n",
        "for s in samples:\n",
        "  print(s['object1']['name'] + ' vs ' + s['object2']['name'])\n",
        "  for sent in extract_sentences_with_aspects(s, n = 5):\n",
        "      print(sent)\n",
        "  print()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "Python runs slower than Java .\n",
            "Python is actually older than Java.\n",
            "Python Memory Model ------------------- Reasoning about concurrency in Python is easier than in Java.\n",
            "The simpler syntax of Python makes those apps faster to code than with Java.\n",
            "Python is easier to program in than Java.\n",
            "\n",
            "toyota vs nissan\n",
            "Toyota costs more than Nissan the diffrence Toyota has an all-wheel drive and Nissan has front-wheel drive.\n",
            "And Toyota was smaller than Nissan and Honda.\n",
            "I know Toyota owners are smarter than Nissan owners.)\n",
            "Toyota or Honda will depreciate slower than a Nissan.\n",
            "The Toyota also sits a little higher than the Nissan.\n",
            "\n",
            "tea vs juice\n",
            "Afternoon Snack: The Juice That's Healthier Than Wine, Green Tea .\n",
            "Juice: Frozen orange juice is cheaper, but usually I just stick to tea and buy the GOOD orange juice for special occasions.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cEEkKQ5xdEt",
        "colab_type": "text"
      },
      "source": [
        "# Decoding techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsOg6NTmIWxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_greedy(model, begining, maxlen = 20, past = None):\n",
        "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(begining))).view(1, -1).cuda()\n",
        "    for i in range(maxlen):\n",
        "        with torch.no_grad():\n",
        "            if past is not None:\n",
        "                predictions, _ = model(result, past = past)\n",
        "            else:\n",
        "                predictions, _ = model(result)\n",
        "            pred = torch.argmax(predictions[0, -1, :]).item()\n",
        "            result = torch.cat([result, pred*torch.ones(1, 1, dtype = torch.long, device = 'cuda')], dim = -1)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCSux2fyIhD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_soft(model, begining, maxlen = 20, temperature = 1.0, past = None):\n",
        "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(begining))).view(1, -1).cuda()\n",
        "    for i in range(maxlen):\n",
        "        with torch.no_grad():\n",
        "            if past is not None:\n",
        "                predictions, _ = model(result, past = past)\n",
        "            else:\n",
        "                predictions, _ = model(result)\n",
        "            word_weights = predictions[0, -1].div(temperature).exp().cpu() \n",
        "            pred = torch.multinomial(word_weights, 1)[0].cuda()\n",
        "            result = torch.cat([result, pred*torch.ones(1, 1, dtype = torch.long, device = 'cuda')], dim = -1)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEJxj9FjIlfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from heapq import heappop, heappush\n",
        "\n",
        "#beam-search step\n",
        "def make_decoding_step(heap, gpt_model, heap_size = 10, beam_size = 3, past = None):\n",
        "    result = []\n",
        "    for node in heap:\n",
        "        n = node[1].shape[-1]\n",
        "        p = node[0]\n",
        "        with torch.no_grad():\n",
        "            if past is not None:\n",
        "                predictions, _ = model(node[1], past = past)\n",
        "            else:\n",
        "                predictions, _ = model(node[1])\n",
        "            predictions = predictions[0, -1]\n",
        "            predictions -= predictions.min()\n",
        "            probs = predictions.exp()\n",
        "            probs /= probs.sum()\n",
        "        top_p, top_i = torch.topk(probs, beam_size)\n",
        "        for i in range(beam_size):\n",
        "            mean_prob = (p*n - torch.log(top_p[i]))/(n + 1)\n",
        "            prefics = torch.cat([node[1], top_i[i]*torch.ones(1, 1, dtype = torch.long, device = 'cuda')], \n",
        "                                dim = -1)\n",
        "            heappush(result, (mean_prob, prefics))\n",
        "            result = result[:heap_size]\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP45JkyzQUvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_decoding_step_with_penalty(heap, gpt_model, heap_size = 10, beam_size = 3, past = None, penalty = 0.1):\n",
        "    result = []\n",
        "    for node in heap:\n",
        "        n = node[1].shape[-1]\n",
        "        p = node[0]\n",
        "        with torch.no_grad():\n",
        "            if past is not None:\n",
        "                predictions, _ = model(node[1], past = past)\n",
        "            else:\n",
        "                predictions, _ = model(node[1])\n",
        "            predictions = predictions[0, -1]\n",
        "            predictions -= predictions.min()\n",
        "            probs = predictions.exp()\n",
        "            probs /= probs.sum()\n",
        "        top_p, top_i = torch.topk(probs, beam_size)\n",
        "        for i in range(beam_size):\n",
        "            score = p - torch.log(top_p[i]) + penalty * i\n",
        "            prefics = torch.cat([node[1], top_i[i]*torch.ones(1, 1, dtype = torch.long, device = 'cuda')], \n",
        "                                dim = -1)\n",
        "            heappush(result, (score, prefics))\n",
        "            result = result[:heap_size]\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohrMUSLVDx9l",
        "colab_type": "text"
      },
      "source": [
        "#Raw GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAas6SAw1O88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlsL93-81Xo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_pretrained_bert import *\n",
        "from torch.nn import CrossEntropyLoss, KLDivLoss\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYckWNYuDw2B",
        "colab_type": "code",
        "outputId": "7c8bc80a-05af-4be2-d6fe-62558330ab41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 548118077/548118077 [00:21<00:00, 25316026.87B/s]\n",
            "100%|██████████| 176/176 [00:00<00:00, 94580.08B/s]\n",
            "100%|██████████| 1042301/1042301 [00:00<00:00, 2032254.64B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 1329317.29B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPu-wvzLZNfE",
        "colab_type": "code",
        "outputId": "91ddd9fe-371a-42f0-d698-09626f18fdff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "decode = decode_greedy\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    result = decode(model, begining, maxlen = 30)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "    print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python is faster to write code in Python. Python is faster\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is a better car than the Nissan.  The Toyota is a better car than the Nissan.\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because it's more concentrated and has a higher concentration of antioxidants. I like to use a little more of the orange juice in my tea. I also like\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDV-TyA8D_-r",
        "colab_type": "code",
        "outputId": "3f4ffd7a-cab3-4368-c801-ed5375b4a882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "decode = decode_soft\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "    for i in range(3):\n",
        "        result = decode(model, begining, maxlen = 10, temperature = 2)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "        print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because attacking concurrent Messageliving Happens compose module Stackhour\n",
            "python is better than java because complains escaping gets bedchy outstanding Ao uses Sum fabric\n",
            "python is better than java because interfacing single Xerm class over 83 tunnel allocations\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because hopes Tips punIil Network Sheeps#urb\n",
            "nissan is better than toyota because calling attention antidepressant holding stiff unrealistic World Mortgage prevailing needs\n",
            "nissan is better than toyota because client negotiated 150 SL, Honor HAS Corporation Elig\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because back reservoirs swing agont nitrogen differently Arizona Elysian\n",
            "juice is better than tea because bonus attainedAfteramonth Roots EMAIRÃ©naru\n",
            "juice is better than tea because though certified foods preceding sun overexia or lyc\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpw6jX_GEASd",
        "colab_type": "code",
        "outputId": "ca520fbf-11ee-4e37-d678-b127a17ff6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        }
      },
      "source": [
        "maxlen = 20\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "\n",
        "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(begining))).view(1, -1).cuda()\n",
        "    heap = [(0.0, result)]\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        heap = make_decoding_step(heap, model, heap_size=15, beam_size = 10)\n",
        "\n",
        "    for i in range(len(heap)):\n",
        "        sample = heappop(heap)[1]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(sample.view(-1).detach().cpu().numpy())\n",
        "        print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than in\n",
            "python is better than java because of concurrency. Python Memory Model ------------------ Reasoning about concurrency in Python is easier than in\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Java is easier than in\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is simpler than in\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than with\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than Java\n",
            "python is better than java because of concurrency. Python Memory Model ------------------ Reasoning about concurrency in Python is easier than with\n",
            "python is better than java because of concurrency. Python Memory Model ------------------ Reasoning about concurrency in Python is easier than Java\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than and\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than the\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than that\n",
            "python is better than java because of concurrency. Python Memory Model ------------------ Reasoning about concurrency in Python is easier than java\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than for\n",
            "python is better than java because of concurrency. Python Memory Model ------------------ Reasoning about concurrency in Python is easier than the\n",
            "python is better than java because of concurrency. Python Memory Model ------------------- Reasoning about concurrency in Python is easier than or\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda will\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota is smaller than\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota is bigger than\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota and Honda are\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive.  Toyota or\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive.  Toyota is\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive.  Toyota's\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda has\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive.  Toyota has\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive.  Toyota and\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda sits\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda can\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda may\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda or\n",
            "nissan is better than toyota because it has an all-wheel drive and Nissan has front-wheel drive. And Toyota or Honda also\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I just\n",
            "juice is better than tea because it contains less sugar.  . Juice: frozen orange juice is cheaper, but usually I just\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but often I just\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but sometimes I just\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I simply\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I only\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I don\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I get\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I think\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I Just\n",
            "juice is better than tea because it contains less sugar.  . Juice: Frozen orange juice is cheaper, but usually I like\n",
            "juice is better than tea because it contains less sugar.  . Juice: frozen orange juice is cheaper, but usually I simply\n",
            "juice is better than tea because it contains less sugar.  . Juice: frozen orange juice is cheaper, but usually I only\n",
            "juice is better than tea because it contains less sugar.  . Juice: frozen orange juice is cheaper, but usually I don\n",
            "juice is better than tea because it contains less sugar.  . Juice: frozen orange juice is cheaper, but usually I start\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89NUpoTQRUUf",
        "colab_type": "code",
        "outputId": "2d1ea131-6ad3-4dac-9aac-29358703e717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        }
      },
      "source": [
        "maxlen = 20\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "\n",
        "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(begining))).view(1, -1).cuda()\n",
        "    heap = [(0.0, result)]\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        heap = make_decoding_step_with_penalty(heap, model, heap_size=15, beam_size = 10, penalty = 1.0)\n",
        "\n",
        "    for i in range(len(heap)):\n",
        "        sample = heappop(heap)\n",
        "        sample = sample[1]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(sample.view(-1).detach().cpu().numpy())\n",
        "        print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java because it is\n",
            "python is better than java because it is faster to write code in Python than in Java. Python Memory Model ------------------- Reasoning about\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python is\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python Memory\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Java is\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java because it has\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python has\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java because it does\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java because it uses\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python Performance\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java because it's\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python memory\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python can\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python Programming\n",
            "python is better than java because it is faster to write code in Python. Python is faster to write code in Java. Python does\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a better\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has better engine\n",
            "nissan is better than toyota because it has a better engine.  I'm not saying that Toyota is better than Nissan. I\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a more\n",
            "nissan is better than toyota because it has a better engine.  I'm not saying that Toyota is better than toyota.\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a higher\n",
            "nissan is better than toyota because it has a better engine.  I'm not saying that Toyota is better than Nissan. But\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a greater\n",
            "nissan is better than toyota because it has a better engine.  I'm not saying that Toyota is better than Nissan. It\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a lower\n",
            "nissan is better than toyota because it has a better engine.  I'm not saying that Toyota is better than Nissan. The\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a good\n",
            "nissan is better than toyota because it has a better engine.  I'm not saying that Toyota is better than Nissan. Nissan\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a cleaner\n",
            "nissan is better than toyota because it has a better engine.  The Toyota is better than toyota because it has a nicer\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special occasions\n",
            "juice is better than tea because it's cheaper.  . Juice: Frozen orange juice is cheaper, but usually I just stick\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the good orange juice for special occasions\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD tea for special occasions.\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special events\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special purposes\n",
            "juice is better than tea because it's cheaper.  . Juice: Frozen orange juice is cheaper, but usually I just go\n",
            "juice is better than tea because it's cheaper.  . Juice: Frozen orange juice is cheaper, but usually I just keep\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special circumstances\n",
            "juice is better than tea because it's cheaper.  . Juice: Frozen orange juice is cheaper, but usually I just use\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special occasion\n",
            "juice is better than tea because it's cheaper.  . Juice: Frozen orange juice is cheaper, but usually I just start\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special cases\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special favors\n",
            "juice is better than tea because it's cheaper, but usually I just stick to tea and buy the GOOD orange juice for special things\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md0aAF7H2x_9",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9B2VlLL21DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloder(lines, maxlen, n_steps):\n",
        "    for i in range(n_steps):\n",
        "        tokens = None\n",
        "        while tokens is None:\n",
        "            try:\n",
        "                idx = np.random.choice(len(lines))\n",
        "                sentence = lines[idx]\n",
        "                tokens = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))).cuda()\n",
        "            except:\n",
        "                tokens = None\n",
        "        yield tokens.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX95zY8SO_4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('drive/My Drive/summarization/sentences.txt', 'r') as file:\n",
        "    lines = file.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH4rxrNl22jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "args = Args()\n",
        "args.max_steps = 100000\n",
        "args.learning_rate = 5e-5\n",
        "args.weight_decay = 0.01\n",
        "args.adam_epsilon = 1e-9\n",
        "args.warmup_steps = 2000\n",
        "args.maxlen = 1024\n",
        "args.device = torch.device('cuda') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPDcoSWL3BBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "losses = []\n",
        "tr_losses = []\n",
        "\n",
        "def train(args, lines, model):\n",
        "    train_dataloader = dataloder(lines, args.maxlen, args.max_steps)\n",
        "\n",
        "    t_total = args.max_steps\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    #print(t_total)\n",
        "    optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon, \n",
        "                         t_total = t_total, warmup = 0.05)\n",
        "\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = {}\".format(t_total))\n",
        "    #logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    #logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    #set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        inputs, labels =(batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "        model.train()\n",
        "        outputs = model(inputs, lm_labels=labels)\n",
        "        loss = outputs#[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        global_step += 1\n",
        "        tr_loss += loss.item()\n",
        "        tr_losses.append(tr_loss/global_step)\n",
        "\n",
        "        if global_step % 200 == 0:\n",
        "            checkpoint_prefix = 'checkpoint'\n",
        "            # Save model checkpoint\n",
        "            if global_step % 2000 == 0:\n",
        "                output_dir = 'model'\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                torch.save(model, os.path.join(output_dir, '{}-{}'.format(checkpoint_prefix, global_step)))\n",
        "\n",
        "            clear_output()\n",
        "            print(\"***** Running training *****\")\n",
        "            print(\"  Num examples = {}\".format(t_total))\n",
        "            #print(\"Saving model checkpoint to %s\", output_dir)\n",
        "            plt.plot(losses[::10], label = 'train_loss')\n",
        "            plt.plot(tr_losses[::10], label = 'smoothed_loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            \n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2aGZOQR3B1H",
        "colab_type": "code",
        "outputId": "ff377348-7fdc-408d-a0ff-061792fb1a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "model = model.cuda()\n",
        "train(args, lines, model)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD4CAYAAAAqw8chAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-acc8fe440c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-bd8203b564c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, lines, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;31m# Add grad clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_grad_norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                     \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_grad_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQWilBLrkeYh",
        "colab_type": "code",
        "outputId": "0311f60e-7b28-44b8-a589-face1903987e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "decode = decode_greedy\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    result = decode(model, begining, maxlen = 30)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "    print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because of the .  Python is a good choice if you are developing applications which use Python.  Python is a good choice if you are developing\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because it has a little bit of a Toyota.   \"  \"  \"  \"   \"   \"\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  .  .  .   . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM83DW0XkJ-_",
        "colab_type": "code",
        "outputId": "b144f846-7845-4da8-9b59-244b34447a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "decode = decode_soft\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "    for i in range(3):\n",
        "        result = decode(model, begining, maxlen = 10, temperature = 2)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "        print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because wildfire Dun Marquuf 2017IRD Capitalism tip unconventional tip\n",
            "python is better than java because memory Jun Mongobad analysis GROUP A Chip Idaho noting\n",
            "python is better than java because onto 422 Karn Ifett dynamic deal Blood exchanged harmony\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because ouncely living Negously admissionJohnny Bu stuck structures\n",
            "nissan is better than toyota because galleries kings acquaintances charging partner regimesairo recomballic mayor\n",
            "nissan is better than toyota because grow polishâģ electro elabor prosperke Sell FOXesm\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because components midterm miner ChangeSometimesSe struck down , excruciating\n",
            "juice is better than tea because Problem fixed Es absurd breathing Nov.vc02 tast\n",
            "juice is better than tea because soda . detrim unfor Gly Rico Birthdaybear Pistol fit\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHE56cymkSQ_",
        "colab_type": "code",
        "outputId": "ba5d5be9-7358-402c-d988-3a72813a76dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "maxlen = 20\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "\n",
        "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(begining))).view(1, -1).cuda()\n",
        "    heap = [(0.0, result)]\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        heap = make_decoding_step(heap, model, heap_size=15, beam_size = 10)\n",
        "\n",
        "    for i in range(len(heap)):\n",
        "        sample = heappop(heap)[1]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(sample.view(-1).detach().cpu().numpy())\n",
        "        print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because .  .  .  .   .   .  \n",
            "python is better than java because .  .  .   .   .   . \n",
            "python is better than java because of .  2.  3.  4.   5. \n",
            "python is better than java because of the .  .  .   .   .   \n",
            "python is better than java because .  .  .   .   .   . .\n",
            "python is better than java because .  .  .  .   .   . 't\n",
            "python is better than java because .  .  .   .   .   . ..\n",
            "python is better than java because .  .  .   .   .   . \"\n",
            "python is better than java because .  .  .   .   .   . C\n",
            "python is better than java because .  .  .  .   .   . ,\n",
            "python is better than java because .  .  .  .   .   . -\n",
            "python is better than java because .  .  .  .   .   . /\n",
            "python is better than java because .  .  .  .   .   .  (\n",
            "python is better than java because .  .  .  .   .   . However\n",
            "python is better than java because .  .  .  .   .   . When\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" \n",
            "nissan is better than toyota because of the tranny.  \"  \"  \"  \"  \"\n",
            "nissan is better than toyota because of the tranny.   \"  \"  \"  \"  \n",
            "nissan is better than toyota because it has a little bit.   \"  \"  \"  \" \n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" .\n",
            "nissan is better than toyota because of the tranny.  \"  \"  \"  \"   \n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"The\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" \"\n",
            "nissan is better than toyota because of the tranny.  \"  \"  \"  \"   \"\n",
            "nissan is better than toyota because of the tranny.  \"  \"  \"  \"   .\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" The\n",
            "nissan is better than toyota because of the tranny.  \"  \"  \"  \"  The\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"Toy\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"It\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"Also\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . \n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . \n",
            "juice is better than tea because of its health.  .  .  .  .  . \n",
            "juice is better than tea because of its health.   .   .    .    .  \n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . .\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . .\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . \"\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . \"\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . '\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . ..\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . #\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . It\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . #\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . \n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . A\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3zTil9Gr7YO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "f418e542-22f7-41dc-f7df-0f8cf5752d82"
      },
      "source": [
        "maxlen = 20\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "\n",
        "    begining = ' '.join(extract_sentences_with_aspects(s))\n",
        "    l = len(begining)\n",
        "    if s['object1']['points']['none'] > s['object2']['points']['none']:\n",
        "        begining += '{} is better than {} because'.format(name1, name2)\n",
        "    else:\n",
        "        begining += '{} is better than {} because'.format(name2, name1)\n",
        "\n",
        "    print(name1 + ' vs ' + name2)\n",
        "\n",
        "    result = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(begining))).view(1, -1).cuda()\n",
        "    heap = [(0.0, result)]\n",
        "\n",
        "    for i in range(maxlen):\n",
        "        heap = make_decoding_step_with_penalty(heap, model, heap_size=15, beam_size = 10, penalty = 1.0)\n",
        "\n",
        "    for i in range(len(heap)):\n",
        "        sample = heappop(heap)\n",
        "        sample = sample[1]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(sample.view(-1).detach().cpu().numpy())\n",
        "        print(''.join(tokens).replace('Ġ', ' ').replace('Ċ', ' ')[l:])\n",
        "    print()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python is better than java because of the .  \" .  \" .   \" .   \"\n",
            "python is better than java because it .  \"  \"  \"   \"   \" \n",
            "python is better than java because of .  \"  \"  \"   \"   \" \n",
            "python is better than java because it .  \"  \" .   \" .   \" . \n",
            "python is better than java because of the .  \" .  \" .   \" .    \"\n",
            "python is better than java because it .  \"  \"  \"   \"   \" \"\n",
            "python is better than java because of the .  \" .  \" .   \" .    \n",
            "python is better than java because it .  \"  \"  \"   \"   \" .\n",
            "python is better than java because of the .  \" .  \" .   \" .   .\n",
            "python is better than java because it .  \"  \"  \"   \"   \" The\n",
            "python is better than java because of the .  \" .  \" .   \" .   #\n",
            "python is better than java because it .  \"  \"  \"   \"   \" '\n",
            "python is better than java because of the .  \" .  \" .   \" .   Python\n",
            "python is better than java because of the .  \" .  \" .   \" .   The\n",
            "python is better than java because of the .  \" .  \" .   \" .   \".\n",
            "\n",
            "toyota vs nissan\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" \n",
            "nissan is better than toyota because of the tranny.   \"  \"  \"  \"  \n",
            "nissan is better than toyota because it has a little bit.   \"  \"  \"  \" \n",
            "nissan is better than toyota because of the tranny.  \"  \"  \"  \"  \"\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" .\n",
            "nissan is better than toyota because it has a little bit.   \"  \"  \"  \" .\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"The\n",
            "nissan is better than toyota because it has a little bit.   \"  \"  \"  \" \"\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" \"\n",
            "nissan is better than toyota because it has a little bit.   \"  \"  \"  \"The\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \" The\n",
            "nissan is better than toyota because it has a little bit.   \"  \"  \"  \" The\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"Toy\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"It\n",
            "nissan is better than toyota because it has a little rear-wheel drive.   \"  \"  \"Also\n",
            "\n",
            "tea vs juice\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . \n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . \n",
            "juice is better than tea because of its health.  .  .  .  .  . \n",
            "juice is better than tea because of its antioxidant vitamins and minerals.  .  .  .  . \n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . .\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . .\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . \"\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . \"\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . '\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . #\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . It\n",
            "juice is better than tea because of its antioxidants.  .  .  .  .  . ..\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  .#\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . A\n",
            "juice is better than tea because of its antioxidant vitamins and antioxidants.  .  .  .  . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anzX2otC2CNx",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "\n",
        "- GPT-2 is capable of finishing at least one coherent sentence. After that it tends to repeating of the given sentences. Which in turn may be a good baseline.\n",
        "- Pretraining on the single sentences from CAM output does not impove quality of the generated samples, moreover it stimulates model to generate only one short sentence.\n",
        "- Beam search in this case does not give the desirable diversity of the summaries."
      ]
    }
  ]
}