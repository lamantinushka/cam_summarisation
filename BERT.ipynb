{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamantinushka/cam_summarisation/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdRvv6ZgvBg0",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n",
        "In this part of the project we investigate potential of BERT architecture in unsupervised summarization task. Similar to GPT-2 being a sequence of transformer decoders, BERT is a self-attentive models consisting of sequentially stacked transformer encoders. There were two versions of BERT released: BERT base (110M parameters) and BERT large (340M parameters). Both of them were pretrained for two tasks: Masked Language Modeling and Next Sentence Prediction on a corpus of ~ 3 Bln words (English books + English Wikipedia). \n",
        "\n",
        "In this project we will use base version consisting of 12 transformer blocks with 12 self-attention heads each. It is suggested to fill in the gaps in tokens sequences of form \\<Object1 -GAP1- Aspect -GAP2- Object2 -GAP3-\\>. Where each GAP is a sequence of [MASK] tokens and number of masks is chosen uniformaly from 1 to 3. \n",
        "\n",
        "Original BERT model is proposed to be for the task for the task, but in our case it may not be relevant for several reasons:\n",
        "- Our aim is to extract of BERT general knowledge about language structure, because task-specific information is already given by the objects and aspects.\n",
        "- BERT domain is already wide as it was trained on English Wikipedia.\n",
        "- Dataset of sentences from CAM answers is pretty small and as a result there is a high risk of overfitting. Having 110M parameters BERT will easily restore sentences from the training set.\n",
        "\n",
        "That is why we use 'bert-base-uncased' model with no finetuning. \n",
        "\n",
        "We use three techniques for decoding:\n",
        "- hard decoding (for every MASK choose the most probable token)\n",
        "- soft decoding (for every MASK choose token from the predicted distribution)\n",
        "- iterative decoding (on each step fill only one gap with the most probable token.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EhgjP2KCrTX",
        "colab_type": "text"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHtkjswuotYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTIlUx7apIE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ARkfn91xBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "for line in open('drive/My Drive/summarization/mined_bow_str.json', 'r'):\n",
        "    data.append(json.loads(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCPGBCMpin9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# usefull function that extracts all supporting sentences from the CAM output\n",
        "def write_sentences(sample, sentences = None):\n",
        "    if sentences is None:\n",
        "        sentences = []\n",
        "    for s in sample['object1']['sentences']:\n",
        "        sentences.append(s['text'] + '\\n')\n",
        "    for s in sample['object2']['sentences']:\n",
        "        sentences.append(s['text'] + '\\n')\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtvS2R591A7Z",
        "colab_type": "text"
      },
      "source": [
        "Let's choose some samples to compare model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi92QgUZse69",
        "colab_type": "code",
        "outputId": "5a86ba6d-2951-4735-e7be-8585122f6745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "samples = [data[4], data[155], data[228]]\n",
        "\n",
        "for s in samples:\n",
        "  print(s['object1']['name'] + ' vs ' + s['object2']['name'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "toyota vs nissan\n",
            "tea vs juice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrsI1ORls14K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "8343afdf-0cec-46ad-a594-b2d07b71c512"
      },
      "source": [
        "for s in samples:\n",
        "  print(s['object1']['name'], ':', s['extractedAspectsObject1'])\n",
        "  print(s['object2']['name'], ':', s['extractedAspectsObject2'])\n",
        "  print()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python : ['simpler', 'older', 'easier to program in', 'bigger', 'libraries', 'higher', 'easier', 'faster to code', 'closer to python', 'easier to read']\n",
            "java : ['higher for java', 'closer to java', 'longer', 'faster', 'stronger']\n",
            "\n",
            "toyota : ['gm', 'easier', 'car', 'se', 'longer', 'veichles', 'smarter', 'easier to park', 'corners', 'dealt']\n",
            "nissan : ['horsepower', 'faster', 'stronger', 'latecomer', 'quality', 'reputation', 'greater', 'wiser']\n",
            "\n",
            "tea : ['cheaper']\n",
            "juice : ['healthier']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cEEkKQ5xdEt",
        "colab_type": "text"
      },
      "source": [
        "# Decoding techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUSBa9iBvyzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_masked_input(object1, object2, aspect, obj1 = True):\n",
        "    if obj1:\n",
        "        object1_ = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(object1)))\n",
        "        object2_ = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(object2)))\n",
        "    else:\n",
        "        object1_ = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(object2)))\n",
        "        object2_ = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(object1)))\n",
        "    aspect = torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(aspect)))\n",
        "    n1 = np.random.randint(1, 4)\n",
        "    n2 = np.random.randint(1, 4)\n",
        "    n3 = np.random.randint(1, 4)\n",
        "    l1 = len(object1_)\n",
        "    l2 = len(object2_)\n",
        "    l3 = len(aspect)\n",
        "    length = n1 + n2 + n3 + l1 + l2 + l3\n",
        "    result = 103 * torch.ones(2 + length, dtype = torch.long).cuda()\n",
        "    result[0] = 101\n",
        "    result[-1] = 102\n",
        "    result[1:l1 + 1] = object1_\n",
        "    result[l1 + n1 + 1: l1 + n1 + l3 + 1] = aspect\n",
        "    result[l1 + n1 + l3 + n2 + 1: -1] = object2_\n",
        "    return result.view(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsOg6NTmIWxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_hard(model, input):\n",
        "    result = input.view(-1)\n",
        "    with torch.no_grad():\n",
        "        preds = model(input).view(-1, 30522)\n",
        "    mask = input == 103\n",
        "    idxes = mask.nonzero().view(-1)\n",
        "    for idx in idxes:\n",
        "        result[idx] = torch.argmax(preds[idx])\n",
        "    return result[1:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCSux2fyIhD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_soft(model, input, temperature = 1.0):\n",
        "    result = input.view(-1)\n",
        "    with torch.no_grad():\n",
        "        preds = model(input).view(-1, 30522)\n",
        "    mask = input == 103\n",
        "    idxes = mask.nonzero().view(-1)\n",
        "    for idx in idxes:\n",
        "        word_weights = preds[idx].div(temperature).exp().cpu() \n",
        "        pred = torch.multinomial(word_weights, 1)[0].cuda()\n",
        "        result[idx] = pred\n",
        "    return result[1:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEJxj9FjIlfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_iteratively(model, begining, maxlen = 20, temperature = 1.0):\n",
        "    result = input.view(-1)\n",
        "    mask = input == 103\n",
        "    idxes = mask.nonzero().view(-1).cpu().numpy()\n",
        "    np.random.shuffle(idxes)\n",
        "    for idx in idxes:\n",
        "        with torch.no_grad():\n",
        "            preds = model(result.view(1, -1)).view(-1, 30522)\n",
        "            result[idx] = torch.argmax(preds[idx])\n",
        "    return result[1:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohrMUSLVDx9l",
        "colab_type": "text"
      },
      "source": [
        "#BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAas6SAw1O88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlsL93-81Xo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pytorch_pretrained_bert import *\n",
        "from torch.nn import CrossEntropyLoss, KLDivLoss\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYckWNYuDw2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a1c65fe-3085-4f84-9fd9-a369e97d57e2"
      },
      "source": [
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased').cuda()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:13<00:00, 29950466.41B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 907920.79B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrr_3bp9xB7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28826012-3d90-4bda-81cb-6128031d108e"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(['[SEP]'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ3lZUkcO2Wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "7657bfaf-7079-440a-8da2-95b82aac1400"
      },
      "source": [
        "decode = decode_hard\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "    aspects = {a: True for a in s['extractedAspectsObject1']}\n",
        "    for a in s['extractedAspectsObject2']:\n",
        "        aspects[a] = False\n",
        "\n",
        "    sentences = []\n",
        "    for a in aspects:\n",
        "        input = gen_masked_input(name1, name2, a, obj1 = aspects[a])\n",
        "        result = decode(model, input)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "        sentences.append(' '.join(tokens).replace(' ##', ''))\n",
        "  \n",
        "    print(name1 + ' vs ' + name2)\n",
        "    for sent in sentences:\n",
        "        print(sent)\n",
        "    print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python ( simpler java java python java java\n",
            "python java is the older versions versions of java java\n",
            "python is easier to program in parallel ) java java java\n",
            "python is is bigger than java java\n",
            "python java java libraries java java java java java java java\n",
            "python java java higher ) java ) java java java java\n",
            "python is easier than java java java java\n",
            "python is is is faster to code than ) java java java\n",
            "python ( is closer to python ) java java java java\n",
            "python is is easier to read than java java java\n",
            "java and higher for java and python python\n",
            "java is closer to java . . : python python python\n",
            "java ( ( ( longer ) python python python\n",
            "java ( ( , faster ) python python\n",
            "java ( ( ( stronger ) ) python python python python\n",
            "\n",
            "toyota vs nissan\n",
            "toyota nissan nissan gm nissan nissan nissan nissan nissan\n",
            "toyota - easier to nissan nissan nissan nissan nissan\n",
            "toyota toyota nissan car nissan nissan nissan nissan nissan nissan nissan\n",
            "toyota nissan se nissan nissan nissan nissan nissan nissan nissan\n",
            "toyota nissan no longer nissan nissan nissan nissan nissan nissan nissan\n",
            "toyota nissan veichles nissan nissan nissan nissan nissan nissan\n",
            "toyota nissan nissan smarter nissan nissan nissan nissan\n",
            "toyota ( easier to park nissan nissan nissan nissan nissan\n",
            "toyota nissan toyota corners nissan nissan nissan nissan nissan\n",
            "toyota nissan nissan nissan dealt with nissan nissan nissan\n",
            "nissan toyota 300 5 horsepower nissan nissan nissan toyota toyota toyota\n",
            "nissan toyota faster toyota toyota toyota toyota toyota\n",
            "nissan diesel ( stronger nissan toyota toyota toyota toyota\n",
            "nissan leaf toyota nissan latecomer nissan toyota toyota toyota\n",
            "nissan diesel quality nissan nissan toyota toyota toyota toyota\n",
            "nissan has nissan its reputation toyota toyota toyota toyota toyota\n",
            "nissan motors greater tokyo toyota toyota toyota\n",
            "nissan - wiser nissan nissan toyota toyota\n",
            "\n",
            "tea vs juice\n",
            "tea juice cheaper than juice juice\n",
            "juice tea tea the healthier tea tea tea tea\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qofx9B_dO7Rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "1f417b94-9019-4479-fcce-d6f7b6ef4353"
      },
      "source": [
        "decode = decode_soft\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "    aspects = {a: True for a in s['extractedAspectsObject1']}\n",
        "    for a in s['extractedAspectsObject2']:\n",
        "        aspects[a] = False\n",
        "\n",
        "    sentences = []\n",
        "    for a in aspects:\n",
        "        input = gen_masked_input(name1, name2, a, obj1 = aspects[a])\n",
        "        result = decode(model, input)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "        sentences.append(' '.join(tokens).replace(' ##', ''))\n",
        "  \n",
        "    print(name1 + ' vs ' + name2)\n",
        "    for sent in sentences:\n",
        "        print(sent)\n",
        "    print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python ( ( the simpler . in java java\n",
            "python ( ( & older ) classes java java java\n",
            "python , ( is easier to program in c java java java java\n",
            "python web java python bigger ; java java java java java\n",
            "python java extended python libraries . java java java java\n",
            "pythoned @ q higher type java java\n",
            "python c ; easier than java java java java\n",
            "python java these is faster to code python source java java java java\n",
            "python is is closer to python : - java java java java\n",
            "python almost is easier to read ) java java java\n",
            "java is higher for java , : in python python python\n",
            "java ( is closer to java ) python python python python\n",
            "java no longer than python python python\n",
            "java ( faster schedule denim python python python\n",
            "java is ( stronger ) python python\n",
            "\n",
            "toyota vs nissan\n",
            "toyota columbia chrysler gm nissan nissan nissan nissan\n",
            "toyota suspension is - easier than the nissan nissan nissan nissan\n",
            "toyota stock car nissan bmw nissan nissan\n",
            "toyota nissan seso hyundai nissan nissan nissan\n",
            "toyota cunningham , no longer running nissan nissan\n",
            "toyota golf nissan veichles toyota nissan nissan nissan\n",
            "toyota - smarter nissan nissan nissan nissan\n",
            "toyota ford ( easier to park on nissan nissan nissan nissan nissan nissan\n",
            "toyota judgment toyota vehicles corners nissan nissan nissan nissan\n",
            "toyota grid while dealt pam nissan nissan nissan\n",
            "nissan zero horsepower honda toyota toyota toyota\n",
            "nissan leaf diesel faster chevy toyota toyota toyota\n",
            "nissan abraham stronger toyota ford toyota toyota toyota\n",
            "nissan benjamin - nissan latecomer nissan toyota toyota toyota toyota\n",
            "nissan ( quality honda micro nissan toyota toyota toyota\n",
            "nissan falcon reputationi toyota toyota\n",
            "nissan polar - ' greater aqua toyota toyota toyota toyota\n",
            "nissan toyota mitsubishi wiser nissan toyota toyota toyota toyota\n",
            "\n",
            "tea vs juice\n",
            "tea shop or , cheaper orange juice juice\n",
            "juice made tea , healthier tea tea tea tea\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPu-wvzLZNfE",
        "colab_type": "code",
        "outputId": "68dd49ba-5858-463d-d865-2700b4f9b464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "decode = decode_iteratively\n",
        "\n",
        "for s in samples:\n",
        "    name1 = s['object1']['name']\n",
        "    name2 = s['object2']['name']\n",
        "    aspects = {a: True for a in s['extractedAspectsObject1']}\n",
        "    for a in s['extractedAspectsObject2']:\n",
        "        aspects[a] = False\n",
        "\n",
        "    sentences = []\n",
        "    for a in aspects:\n",
        "        input = gen_masked_input(name1, name2, a, obj1 = aspects[a])\n",
        "        result = decode(model, input)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result.view(-1).detach().cpu().numpy())\n",
        "        sentences.append(' '.join(tokens).replace(' ##', ''))\n",
        "  \n",
        "    print(name1 + ' vs ' + name2)\n",
        "    for sent in sentences:\n",
        "        print(sent)\n",
        "    print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python vs java\n",
            "python ( simpler language ) java java java\n",
            "python java ( and older ) java java java\n",
            "python is also much easier to program in . net java java java\n",
            "python java bigger java java java java java java java\n",
            "python . java libraries . java java\n",
            "python java higher . python python java java\n",
            "python java easier than java java java java\n",
            "python java is much faster to code in . java java java java\n",
            "python is closer to python than to java java java\n",
            "python . java is easier to read than . java java\n",
            "javascript or higher for javascript . python python python\n",
            "java is also much closer to java . python python python python python python\n",
            "java ( longer ) python python python python\n",
            "java ( is faster ) . python python python\n",
            "java python python - stronger java python python python python\n",
            "\n",
            "toyota vs nissan\n",
            "toyota nissan gm nissan gm nissan nissan nissan nissan\n",
            "toyota nissan ( much easier ) nissan nissan nissan nissan\n",
            "toyota kei - car nissan nissan nissan nissan nissan nissan\n",
            "toyota nissan nissan se nissan nissan nissan nissan\n",
            "toyota nissan longer wheelbase nissan nissan\n",
            "toyota toyota veichles toyota crown mazda nissan nissan nissan\n",
            "toyota nissan smarter nissan nissan nissan nissan nissan\n",
            "toyota nissan it is easier to park than nissan nissan nissan\n",
            "toyota nissan nissan nissan corners nissan nissan nissan nissan nissan\n",
            "toyota nissan nissan nissan dealt with nissan nissan nissan\n",
            "nissan diesel 5 horsepower toyota diesel toyota toyota toyota toyota\n",
            "nissan toyota - faster toyota - toyota toyota\n",
            "nissan toyota toyota stronger toyota toyota toyota toyota toyota\n",
            "nissan patrol latecomer nissan patrol toyota toyota toyota\n",
            "nissan skyline nissan toyota quality nissan toyota toyota toyota toyota\n",
            "nissan has earned its reputation from the toyota toyota toyota toyota\n",
            "nissan toyota toyota toyota greater distance toyota toyota toyota toyota\n",
            "nissan toyota toyota toyota wiser nissan nissan toyota toyota\n",
            "\n",
            "tea vs juice\n",
            "tea juice juice is cheaper than the tea juice juice juice\n",
            "juice tea tea tea healthier juice tea tea tea\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6kVW465FVgi",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "- Interative generation gives relatively consistent sentences, but they are not very distinct and probably can be generated using templates.\n",
        "- Probably 3 gaps is too much\n",
        "\n",
        "## TBD:\n",
        "- finetuning for MLM task on CAM outputs (?)\n",
        "- some beam-search technique for iterative generation"
      ]
    }
  ]
}